{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54473ac6",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711225bb",
   "metadata": {},
   "source": [
    "__LightGBM__ is a gradient boosting framework that uses tree based learning algorithms. It is designed and efficient with the following advantages:\n",
    "\n",
    "\n",
    "- faster training speed and higher efficiency;\n",
    "\n",
    "\n",
    "- lower memory usage;\n",
    "\n",
    "\n",
    "- better accuracy;\n",
    "\n",
    "\n",
    "- support of parallel and GPU learning;\n",
    "\n",
    "\n",
    "- capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c903fb",
   "metadata": {},
   "source": [
    "## LightGBM intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0dd73",
   "metadata": {},
   "source": [
    "__LightGBM__ grows tree vertically while other tree based learning algorithms grow trees horizontally. \n",
    "\n",
    "\n",
    "- It means that LightGBM grows tree leaf-wise while other algorithms grow level-wise. It will choose the leaf with the max delta loss to grow. When growing the same leaf, leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n",
    "\n",
    "\n",
    "- so, we need to understand the distinction between leaf-wise tree growth and level-wise tree growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b127754",
   "metadata": {},
   "source": [
    "### Important points about tree-growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd22d67",
   "metadata": {},
   "source": [
    "- If we grow the full tree, __best-first (leaf-wise)__ and __depth-first (level_wise)__ will result in the same tree. The difference is in the order in which the tree is expanded. Since we don't normally grow trees to their full depth, order matters. \n",
    "\n",
    "\n",
    "- Application of early stopping criteria and pruning methods can result in very different trees. Because leaf-wise chooses splits based on their contribution to the global loss and not just the loss along a particular branch, it often (not always) will learn lower-error trees 'faster' than level-wise.\n",
    "\n",
    "\n",
    "- For a small number of nodes, leaf-wise will probably out-perform level-wise. As we add more nodes, without stopping or pruning they will converge to the same performance because they will literally build the same tree eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6a49d",
   "metadata": {},
   "source": [
    "## LightGBM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e907ef",
   "metadata": {},
   "source": [
    "### Control Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625fa24",
   "metadata": {},
   "source": [
    "- __max_depth__: it describes the maximum tree depth. This parameter is used to handle overfitting. If the model is overfitted, you should lower max_depth.\n",
    "\n",
    "\n",
    "- __min_data_in_leaf__: it's the minimum number of records a leaf may have. The default value is 20, optimal value. It's also used to deal with overfitting.\n",
    "\n",
    "\n",
    "- __feature_fraction__: used when your boosting is random forest. 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees.\n",
    "\n",
    "\n",
    "- __bagging_fraction__: specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting. \n",
    "\n",
    "\n",
    "- __early_stopping_round__: this parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn't improve in last early_stopping_round rounds. This will reduce excessive iterations.\n",
    "\n",
    "\n",
    "- __lambda__: lambda specifies regularization. Typical value ranges from 0 to 1.\n",
    "\n",
    "\n",
    "- __min_gain_to_split__: This parameter will describe the minimum gain to make a split. It can be used to control the number of useful splits in the tree.\n",
    "\n",
    "\n",
    "- __max_cat_group__: when the number of categories is big, finding the split point on it is easily over-fitting. So LightGBM merges them into 'max_cat_group' groups, and finds the split points on the group boundaries, default:64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd37685",
   "metadata": {},
   "source": [
    "### Core Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ff90b",
   "metadata": {},
   "source": [
    "- __Task__: it specifies the task you want to perform on data. It may be either train or predict.\n",
    "\n",
    "\n",
    "- __application__: specifies the application of your model, whether it is a regression problem or a classification problem. LightGBM will by default consider the model as a regression model.\n",
    "    \n",
    "    - regression;\n",
    "    - classification: binary classification;\n",
    "    - multiclass: for multiclass classification\n",
    "    \n",
    "    \n",
    "- __boosting__: defines the type of algorithm you want to run, default=gdbt.\n",
    "\n",
    "\n",
    "    - gbdt: traditional Gradient Boosting Decision Tree;\n",
    "    - rf: random forest;    \n",
    "    - dart: Dropouts meet Multiple Additive Regression Trees;    \n",
    "    - goss: Gradient-based One-Side Sampling.\n",
    "    \n",
    "    \n",
    "- __num_boost_round__: Number of boosting iterations, typically 100+;\n",
    "\n",
    "\n",
    "- __learning_rate__: this determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitudeof this change in the estimates. Typical values: 0.1, 0.001, 0.003...\n",
    "\n",
    "\n",
    "- __num_leaves__: number of leaves in the full tree, default:31;\n",
    "\n",
    "\n",
    "- __device__: default: cpu, can also pass gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35970d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
